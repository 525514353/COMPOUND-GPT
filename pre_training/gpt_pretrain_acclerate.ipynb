{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:08.584075Z",
     "end_time": "2024-04-21T09:31:09.301503Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk\n",
    "# \\t is the tab character in Python\n",
    "dataset = load_from_disk(r'D:\\system\\桌面\\lcm-code\\tokenizers_lcm\\dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer=GPT2TokenizerFast.from_pretrained(r'D:\\system\\桌面\\lcm-code\\tokenizers_lcm\\tokenizer_gpt100.json')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.303503Z",
     "end_time": "2024-04-21T09:31:10.726610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:10.722610Z",
     "end_time": "2024-04-21T09:31:11.076609Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"isosmiles\"],\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True,remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "length=[len(i) for i in tokenized_dataset['train']['input_ids']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:11.078610Z",
     "end_time": "2024-04-21T09:31:13.522497Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_dataset=tokenized_dataset.filter(lambda example:len(example['input_ids'])<500)\n",
    "tokenized_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:13.523498Z",
     "end_time": "2024-04-21T09:31:13.538498Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits,attention_mask):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:]\n",
    "    shift_logits = logits[..., :-1, :]\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduction='none')\n",
    "    loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "    loss *= attention_mask[:,1:].reshape(-1)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:13.538498Z",
     "end_time": "2024-04-21T09:31:13.583204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:13.556231Z",
     "end_time": "2024-04-21T09:31:13.590204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=20, shuffle=True,collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"val\"], batch_size=20,collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=20,collate_fn=data_collator)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:13.570205Z",
     "end_time": "2024-04-21T09:31:13.590204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_improve(dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            logits=model(batch['input_ids'],attention_mask=batch['attention_mask']).logits\n",
    "            loss=keytoken_weighted_loss(batch['input_ids'],logits,batch['attention_mask'])\n",
    "\n",
    "        losses.append(loss.unsqueeze(dim=0))\n",
    "    loss = torch.mean(torch.cat(losses,dim=0))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss.item(), perplexity.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:13.586206Z",
     "end_time": "2024-04-21T09:31:13.601205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def precise(dataloader):\n",
    "    model.eval()\n",
    "    corrects=0\n",
    "    totals=0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"],attention_mask=batch['attention_mask']).logits[..., :-1, :]\n",
    "\n",
    "            labels=batch[\"input_ids\"][..., 1:][batch['attention_mask'][:,1:].bool()].flatten()\n",
    "\n",
    "            predict=outputs.argmax(dim=2)[batch['attention_mask'][:,1:].bool()].flatten()\n",
    "\n",
    "            correct=(labels==predict).sum()\n",
    "            total=len(labels)\n",
    "        corrects+=correct\n",
    "        totals+=total\n",
    "    torch.cuda.empty_cache()\n",
    "    return corrects/totals"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:13.602204Z",
     "end_time": "2024-04-21T09:31:13.621205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# precise(eval_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:13.617205Z",
     "end_time": "2024-04-21T09:31:13.633205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "# config = AutoConfig.from_pretrained(\n",
    "#     \"gpt2\",\n",
    "#     vocab_size=len(tokenizer),\n",
    "#     bos_token_id=tokenizer.bos_token_id,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "# model = GPT2LMHeadModel(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:13.633205Z",
     "end_time": "2024-04-21T09:31:14.461812Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('chem_gpt100')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:14.461812Z",
     "end_time": "2024-04-21T09:31:14.684456Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:14.687456Z",
     "end_time": "2024-04-21T09:31:14.699702Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "Accelerator.mixed_precision == 'fp16'\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader,test_dataloader\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:14.700657Z",
     "end_time": "2024-04-21T09:31:14.874032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_improve(eval_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:58:57.911192Z",
     "end_time": "2024-04-21T09:59:29.594285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:58:54.783442Z",
     "end_time": "2024-04-21T09:58:55.044897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 30\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:14.904459Z",
     "end_time": "2024-04-21T09:31:14.919459Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name,create_repo\n",
    "\n",
    "model_name = \"chem_gpt100\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name\n",
    "# create_repo(repo_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:14.920460Z",
     "end_time": "2024-04-21T09:31:16.524063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir = \"chem_gpt100\"\n",
    "# repo = Repository(output_dir, clone_from=repo_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:16.525063Z",
     "end_time": "2024-04-21T09:31:16.568228Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "samples_per_step=25\n",
    "gradient_accumulation_steps = 100\n",
    "eval_steps = 200\n",
    "\n",
    "model.train()\n",
    "completed_steps = 1\n",
    "min_eval=0.160\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "perplexcitys=[]\n",
    "\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1),total=len(train_dataloader)\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"],attention_mask=batch['attention_mask']).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, batch['attention_mask'])\n",
    "        if step % (gradient_accumulation_steps) == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"samples\": step * samples_per_step,\n",
    "                    \"iter_steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item(),\n",
    "                }\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    eval_loss, perplexity = evaluate_improve(eval_dataloader)\n",
    "    accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "    model.train()\n",
    "    eval_losses.append(eval_loss)\n",
    "    perplexcitys.append(perplexity)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    if eval_losses[-1]<min_eval:\n",
    "        # accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir)\n",
    "        # model.save_pretrained('chem_gpt')\n",
    "        min_eval=eval_losses[-1]\n",
    "\n",
    "\n",
    "            # if accelerator.is_main_process:\n",
    "            #     tokenizer.save_pretrained(output_dir)\n",
    "            #     repo.push_to_hub(\n",
    "            #         commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "            #     )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-19T23:41:05.013284Z",
     "end_time": "2024-04-20T19:48:11.055174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.style.use('seaborn-darkgrid')  # 设置图表样式\n",
    "\n",
    "plt.plot(range(len(train_losses)), train_losses, label='Train Loss', linewidth=2)\n",
    "plt.plot([i*100 for i in range(31)][1:], eval_losses, label='Eval Loss', linewidth=2)\n",
    "\n",
    "plt.xlabel('Iterations', fontsize=14)  # 设置X轴标签和字体大小\n",
    "plt.ylabel('Loss', fontsize=14)  # 设置Y轴标签和字体大小\n",
    "plt.title('Train Losses', fontsize=16)  # 设置标题和字体大小\n",
    "plt.legend(fontsize=12)  # 设置图例字体大小\n",
    "plt.grid(True)\n",
    "plt.gca().yaxis.set_major_formatter(ticker.FormatStrFormatter('%.4f'))  # 设置Y轴刻度格式\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-20T19:52:19.061348Z",
     "end_time": "2024-04-20T19:52:19.153005Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 将列表写入文本文件中\n",
    "with open('eval.txt', 'w') as f:\n",
    "    for item in eval_losses:\n",
    "        f.write(str(item) +','+ '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-20T20:20:41.311333Z",
     "end_time": "2024-04-20T20:20:41.338803Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-20T20:20:25.639880Z",
     "end_time": "2024-04-20T20:20:25.663621Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T10:00:21.106248Z",
     "end_time": "2024-03-17T10:00:21.659968Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained('chem_gpt100')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T10:01:08.068866Z",
     "end_time": "2024-04-21T10:01:08.361213Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
